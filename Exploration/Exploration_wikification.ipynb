{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring wikifier (\"From Text to Knowledge: The Information Extraction Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of the ``wikifier`` , which outputs entities within a text with their corresponding Wikidata IDs. As input, I utilise a dummy text from english Wikipedia: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"François-Marie Arouet , known by his nom de plume Voltaire, was a French Enlightenment writer, historian, and philosopher famous for his wit, his criticism of Christianity—especially the Roman Catholic Church—as well as his advocacy of freedom of speech, freedom of religion, and separation of church and state.Voltaire's next play, Artémire, set in ancient Macedonia, opened on 15 February 1720. It was a flop and only fragments of the text survive. He instead turned to an epic poem about Henry IV of France that he had begun in early 1717.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "François-Marie Arouet , known by his nom de plume Voltaire, was a French Enlightenment writer, historian, and philosopher famous for his wit, his criticism of Christianity—especially the Roman Catholic Church—as well as his advocacy of freedom of speech, freedom of religion, and separation of church and state.Voltaire's next play, Artémire, set in ancient Macedonia, opened on 15 February 1720. It was a flop and only fragments of the text survive. He instead turned to an epic poem about Henry IV of France that he had begun in early 1717.\n"
     ]
    }
   ],
   "source": [
    "print (text) # checking if the text is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp =  spacy.load('en_core_web_md') # the package has to be installed via Anaconda shell  >python -m spacy download en_core_web_md. You can now load the model via spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Anaconda Shell and install the following packages: \n",
    "``` \n",
    "conda install spacy\n",
    "conda install neuralcoref```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 112 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "# Load SpaCy\n",
    "nlp =  spacy.load('en_core_web_md')\n",
    "# Add neural coref to SpaCy's pipe\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "def coref_resolution(text):\n",
    "    \"\"\"Function that executes coreference resolution on a given text\"\"\"\n",
    "    doc = nlp(text)\n",
    "    # fetches tokens with whitespaces from spacy document\n",
    "    tok_list = list(token.text_with_ws for token in doc)\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        # get tokens from representative cluster name\n",
    "        cluster_main_words = set(cluster.main.text.split(' '))\n",
    "        for coref in cluster:\n",
    "            if coref != cluster.main:  # if coreference element is not the representative element of that cluster\n",
    "                if coref.text != cluster.main.text and bool(set(coref.text.split(' ')).intersection(cluster_main_words)) == False:\n",
    "                    # if coreference element text and representative element text are not equal and none of the coreference element words are in representative element. This was done to handle nested coreference scenarios\n",
    "                    tok_list[coref.start] = cluster.main.text + \\\n",
    "                        doc[coref.end-1].whitespace_\n",
    "                    for i in range(coref.start+1, coref.end):\n",
    "                        tok_list[i] = \"\"\n",
    "\n",
    "    return \"\".join(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_resolution(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
    "\n",
    "def wikifier(text, lang=\"en\", threshold=0.8):\n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", lang),\n",
    "        (\"userKey\", \"tgbdmkpmkluegqfbawcwjywieevmza\"),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"nTopDfValuesToIgnore\", \"100\"), (\"nWordsToIgnoreFromList\", \"100\"),\n",
    "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
    "        (\"support\", \"true\"), (\"ranges\", \"false\"), (\"minLinkFrequency\", \"2\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # Call the Wikifier and read the response.\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # Output the annotations.\n",
    "    results = list()\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        # Filter out desired entity classes\n",
    "        if ('wikiDataClasses' in annotation) and (any([el['enLabel'] in ENTITY_TYPES for el in annotation['wikiDataClasses']])):\n",
    "\n",
    "            # Specify entity label\n",
    "            if any([el['enLabel'] in [\"human\", \"person\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Person'\n",
    "            elif any([el['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Organization'\n",
    "            elif any([el['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Location'\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            results.append({'title': annotation['title'], 'wikiId': annotation['wikiDataItemId'], 'label': label,\n",
    "                            'characters': [(el['chFrom'], el['chTo']) for el in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Voltaire',\n",
       "  'wikiId': 'Q9068',\n",
       "  'label': 'Person',\n",
       "  'characters': [(0, 20), (50, 57), (50, 58)]},\n",
       " {'title': 'Christianity',\n",
       "  'wikiId': 'Q5043',\n",
       "  'label': 'Organization',\n",
       "  'characters': [(159, 170)]},\n",
       " {'title': 'Catholic Church',\n",
       "  'wikiId': 'Q9592',\n",
       "  'label': 'Organization',\n",
       "  'characters': [(183, 207),\n",
       "   (187, 191),\n",
       "   (187, 200),\n",
       "   (187, 207),\n",
       "   (193, 200),\n",
       "   (193, 207),\n",
       "   (202, 207),\n",
       "   (266, 273),\n",
       "   (294, 299)]},\n",
       " {'title': 'Henry IV of France',\n",
       "  'wikiId': 'Q936976',\n",
       "  'label': 'Person',\n",
       "  'characters': [(491, 498), (491, 508)]}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = wikifier(text, lang=\"en\", threshold=0.8) # get all the entities within the Text and their wikidataID\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Exploration of the following pipeline: https://towardsdatascience.com/from-text-to-knowledge-the-information-extraction-pipeline-b65e7e30273e. A dummy text (\"Voltaire\" on Wikipedia) is taken as a test. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
